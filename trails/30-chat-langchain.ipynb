{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818e8a37",
   "metadata": {},
   "source": [
    "# 30 - Chat: RAG-assisted chat loop using Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e006572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env loaded. Pinecone key present: True Index: medical-knowledge\n",
      "langchain version: 1.0.5\n",
      "langchain_huggingface available\n",
      "StructuredOutputParser NOT available - will fallback to manual JSON extraction\n",
      "Loaded local embedder. Dim: 384\n"
     ]
    }
   ],
   "source": [
    "# Cell A - env, import detection\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os, json, re, time, traceback\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX = os.getenv(\"PINECONE_INDEX\",\"medical-knowledge\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\", None)\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\",\"http://127.0.0.1:11434\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\",\"llama3\")\n",
    "\n",
    "print(\"Env loaded. Pinecone key present:\", bool(PINECONE_API_KEY), \"Index:\", PINECONE_INDEX)\n",
    "\n",
    "# LangChain detection (some installs vary)\n",
    "HAS_LANGCHAIN = False\n",
    "HAS_LC_COMMUNITY = False\n",
    "HAS_LC_HF = False\n",
    "HAS_STRUCT_PARSER = False\n",
    "try:\n",
    "    import langchain\n",
    "    HAS_LANGCHAIN = True\n",
    "    print(\"langchain version:\", getattr(langchain,\"__version__\",\"?\"))\n",
    "except Exception:\n",
    "    print(\"langchain not available\")\n",
    "\n",
    "# try huggingface adapter\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings as LC_HF_Emb\n",
    "    HAS_LC_HF = True\n",
    "    print(\"langchain_huggingface available\")\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings as LC_HF_Emb\n",
    "        HAS_LC_COMMUNITY = True\n",
    "        print(\"langchain_community embeddings available\")\n",
    "    except Exception:\n",
    "        print(\"No langchain huggingface/community embeddings adapter available\")\n",
    "\n",
    "# structured parser detection\n",
    "try:\n",
    "    from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "    HAS_STRUCT_PARSER = True\n",
    "    print(\"StructuredOutputParser available\")\n",
    "except Exception:\n",
    "    print(\"StructuredOutputParser NOT available - will fallback to manual JSON extraction\")\n",
    "\n",
    "# sentence-transformers local embedding (fallback and for re-embedding)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Loaded local embedder. Dim:\", len(embed_model.encode(\"test\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d942276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import ConversationalRetrievalChain from known locations. CRC will not be available.\n",
      "crc: None | memory: None\n"
     ]
    }
   ],
   "source": [
    "# CRC init (universal import + memory + chain creation)\n",
    "import traceback\n",
    "crc = None\n",
    "memory = None\n",
    "\n",
    "# Universal import attempts for ConversationalRetrievalChain\n",
    "crc_imported = False\n",
    "ConvChain = None\n",
    "try:\n",
    "    from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "    ConvChain = ConversationalRetrievalChain\n",
    "    crc_imported = True\n",
    "    print(\"Imported CRC from langchain.chains\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not crc_imported:\n",
    "    try:\n",
    "        from langchain_experimental.chains import ConversationalRetrievalChain\n",
    "        ConvChain = ConversationalRetrievalChain\n",
    "        crc_imported = True\n",
    "        print(\"Imported CRC from langchain_experimental.chains\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not crc_imported:\n",
    "    try:\n",
    "        from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "        ConvChain = ConversationalRetrievalChain\n",
    "        crc_imported = True\n",
    "        print(\"Imported CRC from legacy location\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not crc_imported:\n",
    "    print(\"Could not import ConversationalRetrievalChain from known locations. CRC will not be available.\")\n",
    "else:\n",
    "    # build memory\n",
    "    try:\n",
    "        from langchain.memory import ConversationBufferMemory\n",
    "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        print(\"ConversationBufferMemory ready\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not import ConversationBufferMemory:\", e)\n",
    "        memory = None\n",
    "\n",
    "    # prepare a retriever object for the chain (use LangChain retriever if present else wrap manual)\n",
    "    chain_retriever = None\n",
    "    try:\n",
    "        if 'retriever' in globals() and retriever is not None:\n",
    "            chain_retriever = retriever\n",
    "            print(\"Using existing LangChain retriever for CRC\")\n",
    "        else:\n",
    "            # simple wrapper exposing get_relevant_documents()\n",
    "            class SimpleRetriever:\n",
    "                def __init__(self, k=3): self.k = k\n",
    "                def get_relevant_documents(self, query):\n",
    "                    ctx = retrieve_context_manual(query, k=self.k)\n",
    "                    class D: \n",
    "                        def __init__(self, text): self.page_content = text\n",
    "                    return [D(ctx)] if ctx else []\n",
    "            chain_retriever = SimpleRetriever(k=3)\n",
    "            print(\"Using SimpleRetriever wrapper (manual retrieval) for CRC\")\n",
    "    except Exception as e:\n",
    "        print(\"Error preparing retriever:\", e)\n",
    "        chain_retriever = None\n",
    "\n",
    "    # instantiate CRC\n",
    "    try:\n",
    "        if ConvChain is not None and ollama_llm is not None and chain_retriever is not None:\n",
    "            crc = ConvChain.from_llm(llm=ollama_llm, retriever=chain_retriever, memory=memory, return_source_documents=False, verbose=False)\n",
    "            print(\"ConversationalRetrievalChain initialized successfully.\")\n",
    "        else:\n",
    "            print(\"Missing components for CRC (ConvChain/LLM/retriever). CRC not created.\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to construct CRC:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# at the end we have: crc (chain or None), memory (or None)\n",
    "print(\"crc:\", \"available\" if crc else \"None\", \"| memory:\", \"available\" if memory else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736e5d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected via Pinecone() client -> Index OK\n",
      "LangChain retriever init failed or not available: module 'pinecone' has no attribute 'Index'\n",
      "Retriever mode: manual\n"
     ]
    }
   ],
   "source": [
    "# Cell B - robust Pinecone initialization & retriever setup\n",
    "index = None\n",
    "pc = None\n",
    "try:\n",
    "    from pinecone import Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(PINECONE_INDEX)\n",
    "    print(\"Connected via Pinecone() client -> Index OK\")\n",
    "except Exception:\n",
    "    try:\n",
    "        import pinecone\n",
    "        try:\n",
    "            if PINECONE_ENV:\n",
    "                pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "            else:\n",
    "                pinecone.init(api_key=PINECONE_API_KEY)\n",
    "        except Exception:\n",
    "            pass\n",
    "        index = pinecone.Index(PINECONE_INDEX)\n",
    "        pc = pinecone\n",
    "        print(\"Connected via classic pinecone.init -> Index OK\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to init Pinecone:\", e)\n",
    "        raise\n",
    "\n",
    "# Build LangChain retriever if available\n",
    "retriever = None\n",
    "USE_RETRIEVER = \"manual\"\n",
    "try:\n",
    "    if HAS_LC_HF:\n",
    "        lc_emb = LC_HF_Emb(model_name=\"all-MiniLM-L6-v2\")\n",
    "        from langchain_community.vectorstores import Pinecone as LC_Pinecone\n",
    "        vectorstore = LC_Pinecone.from_existing_index(index_name=PINECONE_INDEX, embedding=lc_emb)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "        USE_RETRIEVER = \"langchain\"\n",
    "        print(\"Using langchain-community Pinecone retriever\")\n",
    "except Exception as e:\n",
    "    print(\"LangChain retriever init failed or not available:\", e)\n",
    "    USE_RETRIEVER = \"manual\"\n",
    "\n",
    "# Manual retriever fallback\n",
    "def retrieve_context_manual(query, k=3):\n",
    "    qvec = embed_model.encode(query).tolist()\n",
    "    res = index.query(vector=qvec, top_k=k, include_metadata=True)\n",
    "    matches = res.get(\"matches\", [])\n",
    "    texts = [m.get(\"metadata\",{}).get(\"text\",\"\") for m in matches]\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "print(\"Retriever mode:\", USE_RETRIEVER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ed5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create LangChain LLM wrapper: No module named 'langchain.llms'\n"
     ]
    }
   ],
   "source": [
    "# Cell C - robust call_ollama + LangChain LLM wrapper\n",
    "import requests, json, re, html\n",
    "OLLAMA_API = OLLAMA_URL.rstrip('/') + \"/api/generate\"\n",
    "\n",
    "def call_ollama(prompt, max_tokens=512, temperature=0.0, timeout=60):\n",
    "    payload = {\"model\": MODEL_NAME, \"prompt\": prompt, \"max_tokens\": max_tokens, \"temperature\": temperature, \"stream\": False}\n",
    "    r = requests.post(OLLAMA_API, json=payload, timeout=timeout)\n",
    "    text = (r.text or \"\").strip()\n",
    "    # try JSON decode\n",
    "    try:\n",
    "        data = r.json()\n",
    "        if isinstance(data, dict):\n",
    "            if \"response\" in data: return data[\"response\"]\n",
    "            if \"output\" in data:\n",
    "                out = data[\"output\"]\n",
    "                if isinstance(out, list): return \"\".join(p.get(\"content\",\"\") if isinstance(p,dict) else str(p) for p in out)\n",
    "                return str(out)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # NDJSON fallback: collect 'response' lines\n",
    "    out = \"\"\n",
    "    for ln in text.splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        try:\n",
    "            obj = json.loads(ln)\n",
    "            if isinstance(obj, dict) and obj.get(\"response\"):\n",
    "                out += str(obj.get(\"response\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "    if out.strip(): return out.strip()\n",
    "    return text\n",
    "\n",
    "# LangChain LLM wrapper (lightweight)\n",
    "try:\n",
    "    from langchain.llms.base import LLM\n",
    "    from typing import Optional, List, Mapping, Any\n",
    "    class OllamaLLM(LLM):\n",
    "        def __init__(self, model_name: str = MODEL_NAME, temperature: float = 0.0):\n",
    "            self.model_name = model_name\n",
    "            self.temperature = temperature\n",
    "        @property\n",
    "        def _identifying_params(self) -> Mapping[str,Any]:\n",
    "            return {\"model\": self.model_name}\n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return \"ollama\"\n",
    "        def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "            return call_ollama(prompt, max_tokens=512, temperature=self.temperature)\n",
    "    ollama_llm = OllamaLLM()\n",
    "    print(\"OllamaLLM wrapper ready for LangChain.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not create LangChain LLM wrapper:\", e)\n",
    "    ollama_llm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df4d2c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC initialization failed (will fallback to manual loop): No module named 'langchain_experimental.chains'\n"
     ]
    }
   ],
   "source": [
    "# Cell D - Memory + CRC initialization (with fallback)\n",
    "memory = None\n",
    "crc = None\n",
    "try:\n",
    "    # try to import the chain (may require langchain-experimental or latest langchain)\n",
    "    try:\n",
    "        # modern path\n",
    "        from langchain.chains import ConversationalRetrievalChain\n",
    "    except Exception:\n",
    "        from langchain_experimental.chains import ConversationalRetrievalChain\n",
    "\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    if ollama_llm is None:\n",
    "        raise RuntimeError(\"Ollama LLM wrapper missing — CRC init skipped\")\n",
    "    # set up the chain using langchain retriever if available else wrap manual retriever\n",
    "    if USE_RETRIEVER == \"langchain\" and retriever is not None:\n",
    "        chain_retriever = retriever\n",
    "    else:\n",
    "        # define a tiny wrapper class that exposes get_relevant_documents()\n",
    "        class SimpleRetriever:\n",
    "            def __init__(self, k=3):\n",
    "                self.k = k\n",
    "            def get_relevant_documents(self, query):\n",
    "                # return simple objects with page_content property (LangChain expects Document-like)\n",
    "                ctx = retrieve_context_manual(query, k=self.k)\n",
    "                class D:\n",
    "                    def __init__(self, text): self.page_content = text\n",
    "                return [D(ctx)] if ctx else []\n",
    "        chain_retriever = SimpleRetriever(k=3)\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ollama_llm,\n",
    "        retriever=chain_retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"ConversationalRetrievalChain (CRC) initialized.\")\n",
    "except Exception as e:\n",
    "    print(\"CRC initialization failed (will fallback to manual loop):\", e)\n",
    "    crc = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91642a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using manual JSON format instructions fallback.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Structured output parser or manual format instructions\n",
    "parser = None\n",
    "format_instructions = None\n",
    "try:\n",
    "    from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "    schemas = [\n",
    "        ResponseSchema(name=\"symptoms\", description=\"List of reported symptoms\"),\n",
    "        ResponseSchema(name=\"duration\", description=\"Onset/duration (e.g., '3 days')\"),\n",
    "        ResponseSchema(name=\"severity\", description=\"mild/moderate/severe\"),\n",
    "        ResponseSchema(name=\"current_medication\", description=\"list or empty\"),\n",
    "        ResponseSchema(name=\"allergies\", description=\"list or empty\"),\n",
    "        ResponseSchema(name=\"urgency\", description=\"low/medium/high\"),\n",
    "        ResponseSchema(name=\"notes\", description=\"short notes\")\n",
    "    ]\n",
    "    parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    print(\"Using LangChain StructuredOutputParser.\")\n",
    "except Exception:\n",
    "    format_instructions = (\n",
    "        \"Output ONLY a single valid JSON object matching this schema:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"symptoms\\\": [\\\"string\\\", ...],\\n\"\n",
    "        \"  \\\"duration\\\": \\\"string\\\",\\n\"\n",
    "        \"  \\\"severity\\\": \\\"mild/moderate/severe\\\",\\n\"\n",
    "        \"  \\\"current_medication\\\": [\\\"string\\\", ...],\\n\"\n",
    "        \"  \\\"allergies\\\": [\\\"string\\\", ...],\\n\"\n",
    "        \"  \\\"urgency\\\": \\\"low/medium/high\\\",\\n\"\n",
    "        \"  \\\"notes\\\": \\\"string\\\"\\n\"\n",
    "        \"}\\nDo not output any extra explanatory text — ONLY the JSON.\"\n",
    "    )\n",
    "    print(\"Using manual JSON format instructions fallback.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8be9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_next_question ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - ask_next_question: retrieves context then asks a single follow-up question\n",
    "def ask_next_question(user_text, k=3):\n",
    "    if USE_RETRIEVER == \"langchain\" and retriever is not None:\n",
    "        try:\n",
    "            docs = retriever.get_relevant_documents(user_text)\n",
    "            ctx = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        except Exception as e:\n",
    "            print(\"LangChain retriever failed:\", e)\n",
    "            ctx = retrieve_context_manual(user_text, k=k)\n",
    "    else:\n",
    "        ctx = retrieve_context_manual(user_text, k=k)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a concise clinical triage assistant. Using ONLY the context below, \"\n",
    "        \"ask ONE concise follow-up question that will help clarify the patient's symptoms. \"\n",
    "        \"Do NOT provide a diagnosis or recommendations—only a question.\\n\\n\"\n",
    "        f\"Context:\\n{ctx}\\n\\nPatient statement:\\n{user_text}\\n\\nAsk one short question:\"\n",
    "    )\n",
    "    return call_ollama(prompt, max_tokens=200)\n",
    "\n",
    "# quick test (uncomment to run)\n",
    "# print(ask_next_question('I have chest pain and breathlessness since morning.'))\n",
    "print(\"ask_next_question ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f97643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient: I have had chest pain and mild breathlessness since morning.\n",
      "Bot: Is your chest pain constant or does it come and go?\n",
      "Patient reply: its a constant pain\n",
      "Bot: Where is this pain located?\n",
      "Patient reply: near chest\n",
      "Bot: Is it a sharp or dull sensation?\n",
      "Patient reply: sharp\n",
      "\n",
      "--- Conversation ---\n",
      " Patient: I have had chest pain and mild breathlessness since morning.\n",
      "Bot: Is your chest pain constant or does it come and go?\n",
      "Patient: its a constant pain\n",
      "Bot: Where is this pain located?\n",
      "Patient: near chest\n",
      "Bot: Is it a sharp or dull sensation?\n",
      "Patient: sharp\n",
      "\n",
      "Raw model output preview:\n",
      " {\n",
      "  \"symptoms\": [\"chest pain\", \"mild breathlessness\"],\n",
      "  \"duration\": \"constant\",\n",
      "  \"severity\": \"mild\",\n",
      "  \"current_medication\": [],\n",
      "  \"allergies\": [],\n",
      "  \"urgency\": \"low\",\n",
      "  \"notes\": \"\"\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - interactive or simulated multi-turn conversation and final JSON extraction\n",
    "chat_transcript = []\n",
    "\n",
    "# initial patient statement - edit here or use input()\n",
    "user_input = \"I have had chest pain and mild breathlessness since morning.\"\n",
    "print(\"Patient:\", user_input)\n",
    "chat_transcript.append(f\"Patient: {user_input}\")\n",
    "\n",
    "# perform 3 follow-up rounds (edit number if you want)\n",
    "for i in range(3):\n",
    "    bot_q = ask_next_question(user_input, k=3)\n",
    "    print(\"Bot:\", bot_q)\n",
    "    chat_transcript.append(f\"Bot: {bot_q}\")\n",
    "\n",
    "    # in interactive notebook you can type; in non-interactive use defaults\n",
    "    try:\n",
    "        answer = input(\"Patient reply (press Enter to use default simulated reply): \").strip()\n",
    "    except Exception:\n",
    "        answer = \"\"\n",
    "    if not answer:\n",
    "        if i == 0:\n",
    "            answer = \"The pain started this morning and is sharp when I breathe.\"\n",
    "        elif i == 1:\n",
    "            answer = \"I feel dizzy sometimes and a bit nauseous.\"\n",
    "        else:\n",
    "            answer = \"I take no regular medicines and have no known allergies.\"\n",
    "    print(\"Patient reply:\", answer)\n",
    "    chat_transcript.append(f\"Patient: {answer}\")\n",
    "    user_input = answer\n",
    "\n",
    "conversation_text = \"\\n\".join(chat_transcript)\n",
    "print(\"\\n--- Conversation ---\\n\", conversation_text)\n",
    "# build final prompt\n",
    "final_prompt = f\"{format_instructions}\\n\\nConversation:\\n{conversation_text}\\n\\nNow output ONLY the JSON.\"\n",
    "raw_report = call_ollama(final_prompt, max_tokens=700)\n",
    "print(\"\\nRaw model output preview:\\n\", raw_report[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19335fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON object found in model output; storing raw as notes.\n",
      "Triage result: {'specialist': 'General Physician', 'urgency': 'low'}\n",
      "Saved outputs/langchain_report.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 - parse structured JSON (try parser, else manual), triage and save\n",
    "structured = {}\n",
    "if parser is not None:\n",
    "    try:\n",
    "        structured = parser.parse(raw_report)\n",
    "        print(\"Parsed via LangChain parser:\", structured)\n",
    "    except Exception as e:\n",
    "        print(\"LangChain parser failed:\", e)\n",
    "\n",
    "if not structured:\n",
    "    # try to extract first JSON object in response\n",
    "    m = re.search(r'(\\{.*\\})', raw_report, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            structured = json.loads(m.group(1))\n",
    "            print(\"Parsed via manual JSON load.\")\n",
    "        except Exception as e:\n",
    "            print(\"Manual JSON parse failed:\", e)\n",
    "    else:\n",
    "        print(\"No JSON object found in model output; storing raw as notes.\")\n",
    "        structured = {\"symptoms\": [], \"duration\": \"\", \"severity\": \"\", \"current_medication\": [], \"allergies\": [], \"urgency\": \"\", \"notes\": raw_report[:500]}\n",
    "\n",
    "# ensure fields exist\n",
    "for k in [\"symptoms\",\"duration\",\"severity\",\"current_medication\",\"allergies\",\"urgency\",\"notes\"]:\n",
    "    if k not in structured:\n",
    "        structured[k] = [] if k in [\"symptoms\",\"current_medication\",\"allergies\"] else \"\"\n",
    "\n",
    "# simple triage rules\n",
    "def triage_report(s):\n",
    "    symptoms_text = \" \".join(s.get(\"symptoms\", [])) if isinstance(s.get(\"symptoms\", []), list) else str(s.get(\"symptoms\",\"\"))\n",
    "    s_lower = symptoms_text.lower()\n",
    "    if any(w in s_lower for w in [\"chest\",\"breath\",\"shortness\",\"palpit\"]):\n",
    "        return {\"specialist\":\"Cardiology/Emergency\",\"urgency\":\"high\"}\n",
    "    if any(w in s_lower for w in [\"rash\",\"itch\",\"lesion\"]):\n",
    "        return {\"specialist\":\"Dermatology\",\"urgency\":\"medium\"}\n",
    "    if any(w in s_lower for w in [\"headache\",\"dizzy\",\"seizure\"]):\n",
    "        return {\"specialist\":\"Neurology\",\"urgency\":\"medium\"}\n",
    "    return {\"specialist\":\"General Physician\",\"urgency\":\"low\"}\n",
    "\n",
    "triage = triage_report(structured)\n",
    "print(\"Triage result:\", triage)\n",
    "\n",
    "# save to outputs\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "out = {\"conversation\": chat_transcript, \"structured\": structured, \"triage\": triage}\n",
    "with open(\"outputs/langchain_report.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "print(\"Saved outputs/langchain_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2d92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niva-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
